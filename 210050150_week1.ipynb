{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1SIO_6qmk2LEIiC9wyhMngi3oz8A9ESxr","timestamp":1672843624350}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Week 1 : Linear Algebra and Calculus**\n","\n","In this assignment, we shall explore the concepts of analytic and numeric computation of gradients. Further, we will have a look at the computation graph which is a central concept to building a neural network. For learning how gradients are computed analytically, refer to the resources provided in this week."],"metadata":{"id":"lORIgnt_yoZD"}},{"cell_type":"markdown","source":["<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQWsYeD8ZlYxFBB33qIn7bwQvP-KuvLZXJOoA&usqp=CAU\"\n"," style=\"float:center;width:50px;height:50px;\">"],"metadata":{"id":"I-z3YLRv1U_a"}},{"cell_type":"markdown","source":["# **Importing Libraries**\n","Feel free to import any additional libraries required"],"metadata":{"id":"nn_1mKR02Swb"}},{"cell_type":"code","source":["# Import all libraries here\n","import numpy as np\n","import math\n","\n","# Setting the seed for reproducible results\n"],"metadata":{"id":"ugD-twoX2T4N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# *Problem 1(a)*\n","\n","In this problem, we shall be exploring the concepts of analytic and numeric computation of gradients for scalar valued functions. \n","\n","\\begin{equation}\n","z = w^{T}x + b \\\\ \n","\\hat{y} = \\sigma(z) = \\frac{1}{1+e^{-z}}\\\\ \n","L(\\hat{y}, y) = y.log(\\hat{y}) + (1-y).log(1-\\hat{y})\n","\\end{equation}\n","\n","For this set of equations, the vector w, and scalars b, y are to be treated as constants. \\\\\n","\n","Now, let us find the analytic gradient of the function L with respect to the function x. That is, compute $\\frac{\\delta L}{\\delta x}$. Write the answer obtained as code in the function provided."],"metadata":{"id":"LBmB8rFN2XT7"}},{"cell_type":"markdown","source":["Initialise $w$ to a $10 \\times 1$ vector sampled over a standard multivariate gaussian distribution, b to a uniform random variable over the interval $(0, 1)$, y to a bernoulli random variable over $\\{0, 1\\}$"],"metadata":{"id":"qC-5BMVs-abp"}},{"cell_type":"code","source":["# Initialisation : \n","def bernoulli(p):\n","    rvs = np.array([])\n","    if np.random.rand() <= p:\n","        a=1\n","        rvs = np.append(rvs,a)\n","    else:\n","        a=0\n","        rvs = np.append(rvs,a)\n","    return rvs\n","\n","w1 = np.random.multivariate_normal([0,0,0,0,0,0,0,0,0,0], np.identity(10))\n","w2 = np.array(w1)\n","w = np.reshape(w2, (10,1))\n","b = np.random.uniform(0,1)\n","y1 = bernoulli(0.5)\n","y = y1[0]"],"metadata":{"id":"1z1FuiZn-mg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analytic_grad(x) : \n","\n","    ### WRITE CODE BELOW ###\n","    w_transpose=w.transpose()\n","    z=np.dot(w_transpose, x)+b\n","    y_cap=1/(1+math.exp(-z))\n","    deravative=y*y_cap*math.exp(-z)*w-math.exp(-z)*((1-y)/(1-y_cap))*(y_cap*y_cap)*w\n","    return deravative\n","    ### WRITE CODE ABOVE ###\n","\n","w1 = np.random.multivariate_normal([0,0,0,0,0,0,0,0,0,0], np.identity(10))\n","w2 = np.array(w1)\n","x = np.reshape(w2, (10,1))\n","print(analytic_grad(x))"],"metadata":{"id":"rhgmU3v-6jhd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672499484153,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sathwika Reddy somidi","userId":"01697320888258713645"}},"outputId":"e8d1d42d-69fd-4832-fc90-fcb130258e9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.06479532]\n"," [ 0.09140952]\n"," [-0.03754852]\n"," [ 0.00714193]\n"," [-0.02423793]\n"," [-0.01474312]\n"," [-0.05028835]\n"," [ 0.09520192]\n"," [ 0.07167586]\n"," [ 0.00635588]]\n"]}]},{"cell_type":"markdown","source":["# *Problem 1(b)*\n","\n","Now, we compute the numeric gradient for the function L. Refer to [this](https://stackoverflow.com/questions/38854363/is-there-any-standard-way-to-calculate-the-numerical-gradient) stack exchange post to see how numeric gradients are computed"],"metadata":{"id":"YjAm_AKN7Qk-"}},{"cell_type":"code","source":["gradient=np.zeros((10,1))\n","def function_value(x):\n","    w_transpose=w.transpose()\n","    z=np.dot(w_transpose, x)+b\n","    y_cap=1/(1+math.exp(-z))\n","    value=y*math.log(y_cap)+(1-y)*math.log(1-y_cap)\n","\n","    return value\n","\n","def function(x,i):\n","    eps = np.finfo(np.float64).eps\n","    I=np.identity(10)\n","    I[i,i]=I[i,i]*(1+math.sqrt(eps))\n","    x1=np.dot(x.transpose(), I)\n","    value1=function_value(x1.transpose())\n","    I[i,i]=I[i,i]/(1+math.sqrt(eps))\n","    I[i,i]=I[i,i]*(1-math.sqrt(eps))\n","    x2=np.dot(x.transpose(),I)\n","    value2=function_value(x2.transpose())\n","    gradient[i][0]=(value1-value2)/(2*math.sqrt(eps)*x[i][0])\n","\n","    return \n","\n","def numeric_grad(x) : \n","\n","    ### WRITE CODE BELOW ###\n","    for i in range(0,10):\n","      function(x,i)\n","\n","    return gradient\n","    ### WRITE CODE ABOVE ###\n","\n","w1 = np.random.multivariate_normal([0,0,0,0,0,0,0,0,0,0], np.identity(10))\n","w2 = np.array(w1)\n","x = np.reshape(w2, (10,1))\n","print(numeric_grad(x))"],"metadata":{"id":"Ur4CfvOf7YeR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672499511340,"user_tz":-330,"elapsed":402,"user":{"displayName":"Sathwika Reddy somidi","userId":"01697320888258713645"}},"outputId":"fe8cb569-8c2c-4dc4-bf65-ee6bfdec0980"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.17257574]\n"," [ 0.24345994]\n"," [-0.10000666]\n"," [ 0.01902178]\n"," [-0.06455525]\n"," [-0.0392668 ]\n"," [-0.13393789]\n"," [ 0.25356064]\n"," [ 0.19090135]\n"," [ 0.01692824]]\n"]}]},{"cell_type":"markdown","source":["# *Problem 2*\n","\n","Here, we'll be looking at computational graphs, and their calculus, finding gradients of  variables with respect to other variables in the graph.\n","\n"],"metadata":{"id":"MwJI9VYhLjCi"}},{"cell_type":"markdown","source":["We'll be looking at nodes of the graph that are operation based, i.e., each operation performed has a node associated with it.\n","\n","We'll provide you with example implementations for three of the nodes, the *add* node, the *nth power* node and the *sine* node, you'll have to write the classes for all the other nodes."],"metadata":{"id":"0wwEXqQEREOH"}},{"cell_type":"markdown","source":["# **Multi-input nodes**"],"metadata":{"id":"YIeEOTBytyGB"}},{"cell_type":"code","source":["import numpy as np\n","\n","class Add: \n","  \n","  # A class to add multiple variables together\n","  def __init__(self, lst_names, lst_values):\n","    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be added, with each variable name in datatype str.\n","                               # Scalar addition is taken care of in a separate node, we could have fit into this node but thought it might be easier if it wasn't.\n","                               # So cases like \"a + 1\" are to be done separately, and cases like \"b + c + d + 4\" can be done as \"(b + c + d) + 4\" or \"b + c + (d + 4)\", since our scalar addition supports only one variable and one scalar, we'll get to that later\n","    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be added, as scalars.\n","\n","  def compute_output(self):\n","    return np.sum(self.lst_values) # This computes the sum of all the variables\n","  \n","  def compute_gradients(self):\n","    return dict(zip(self.lst_names, np.ones(len(self.lst_names)))) # This gives the gradient of the sum wrt to all the input variables, as a dictionary, will be used later\n","\n","class Multiply: \n","  \n","  #Everything's almost the same as the add class, but this deals with multiplication of more than 1 variables\n","  def __init__(self, lst_names, lst_values):\n","    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be multiplied, with each variable name in datatype str.\n","                               # Scalar multiplication is taken care of in a separate node\n","    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be multiplied, as scalars.\n","\n","  def compute_output(self):\n","    # Write your code to compute the output of this operation\n","    return np.prod(self.lst_values)\n","  \n","  def compute_gradients(self):\n","    product=np.prod(self.lst_values)\n","    array=np.zeros(len(self.lst_names))\n","    for i in range(0, len(self.lst_names)):\n","      array[i]=product/self.lst_values[i]\n","    return dict(zip(self.lst_names, array))\n","    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)\n","\n","A= Multiply([\"a\",\"b\",\"c\"],[1,2,3])\n","print(A.compute_gradients())"],"metadata":{"id":"JQuHj5IctK4N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672511038748,"user_tz":-330,"elapsed":401,"user":{"displayName":"Sathwika Reddy somidi","userId":"01697320888258713645"}},"outputId":"0950523f-bca1-4b08-a184-d6323b38d2ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'a': 6.0, 'b': 3.0, 'c': 2.0}\n"]}]},{"cell_type":"markdown","source":["# **Scalar multiplication/addition nodes**"],"metadata":{"id":"_QpfDyxht7Ob"}},{"cell_type":"code","source":["class Add_Scalar: \n","  \n","  # A class to add a variable with a scalar\n","  def __init__(self, lst_names, lst_values):\n","    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be added, with each variable name in datatype str.\n","                               # This list must only have 2 elements, the first should be the string corresponding to the name of the variable, and the second should be a string of the scalar value to be added.\n","    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be added, as scalars.\n","\n","  def compute_output(self):\n","    # Write your code to compute the output of this operation\n","    return np.sum(self.lst_values)\n","\n","  def compute_gradients(self):\n","    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)\n","    array=np.zeros(len(self.lst_names))\n","    array[0]=1\n","    array[1]=0\n","    return dict(zip(self.lst_names, array))\n","\n","\n","class Multiply_Scalar: \n","  \n","  # A class to multiply a variable with a scalar\n","  def __init__(self, lst_names, lst_values):\n","    self.lst_names = lst_names # This numpy arr contains all the variable names that are to be multiplied, with each variable name in datatype str.\n","                               # This list must only have 2 elements, the first should be the string corresponding to the name of the variable, and the second should be a string of the scalar value to be multiplied.\n","    self.lst_values = lst_values # This numpy arr contains all the variable values that are to be multiplied, as scalars.\n","\n","  def compute_output(self):\n","    # Write your code to compute the output of this operation\n","    return np.prod(self.lst_values)\n","\n","  def compute_gradients(self):\n","    # Write your code to create a dictionary, storing all the gradients of the output wrt to each input (In this case there is only a single input that matters)\n","    array=np.zeros(len(self.lst_names))\n","    array[0]=self.lst_values[1]\n","    array[1]=0\n","    return dict(zip(self.lst_names, array))\n","\n","A= Multiply_Scalar([\"a\",\"4\"],[1,4])\n","print(A.compute_gradients())\n","A= Add_Scalar([\"a\",\"4\"],[1,4])\n","print(A.compute_gradients())\n"],"metadata":{"id":"-0RqnZrStbyI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672511042138,"user_tz":-330,"elapsed":406,"user":{"displayName":"Sathwika Reddy somidi","userId":"01697320888258713645"}},"outputId":"601a167b-08e2-4368-ba10-e4e0bb889c90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'a': 4.0, '4': 0.0}\n","{'a': 1.0, '4': 0.0}\n"]}]},{"cell_type":"markdown","source":["# **Nodes for special functions**"],"metadata":{"id":"iF4EA5CLuKXR"}},{"cell_type":"code","source":["class Power:\n","\n","  def __init__(self, name, value, exponent):\n","    self.name = name # Name of the variable to be exponentiated\n","    self.value = value # Value of the variable\n","    self.exponent = exponent # Value of the exponent\n","  \n","  def compute_output(self):\n","    return np.power(self.value, self.exponent)\n","  \n","  def compute_gradients(self):\n","    return {self.name : self.exponent*(np.power(self.value, (self.exponent - 1)))}\n","\n","class Sine: \n","\n","  # A class to apply the sine function on a variable\n","  def __init__(self, name, value):\n","    self.name = name\n","    self.value = value\n","  \n","  def compute_output(self):\n","    return np.sin(self.value)\n","  \n","  def compute_gradients(self):\n","    return {self.name : np.cos(self.value)}\n","\n","\n","class Logarithm:\n","\n","  # A class to compute the logarithm of a value\n","  def __init__(self, name, value):\n","    self.name = name\n","    self.value = value\n","  \n","  def compute_output(self):\n","    # Write your code here\n","    return np.log(self.value)\n","\n","  def compute_gradients(self):\n","    # Write your code here\n","    return {self.name : 1/(self.value)}\n","\n","class Exponential: \n","\n","  # A class to compute the exponential of a value\n","  def __init__(self, name, value):\n","    self.name = name\n","    self.value = value\n","  \n","  def compute_output(self):\n","    # Write your code here\n","    return np.exp(self.value)\n","\n","  def compute_gradients(self):\n","    # Write your code here\n","    return {self.name : np.exp(self.value)}\n","\n"],"metadata":{"id":"RzhhbukvRBD-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have these classes, let's use them to actually find gradients of complex networks. We finally bring in the idea of a computational graph, which makes it much easier for the gradients to be computed.\n","\n","This is the image of the example graph that we want you to work with. \n","\n","\n","\n"],"metadata":{"id":"BsUmSRPKXau8"}},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://drive.google.com/uc?id=1VtI1lf85bG8cO1u_8l0D0rqVGhwHQtPK\"\n"," width=\"500\" height=\"500\">\n","</div>"],"metadata":{"id":"e2hDABSUr8Px"}},{"cell_type":"markdown","source":["\\begin{equation}\n","d = 3a \\\\ \n","e = abc \\\\ \n","f = sin(c) \\\\ \n","g = exp(e) \\\\ \n","h = a + d + g + f\n","\\end{equation}"],"metadata":{"id":"tPKf5T4lrjxY"}},{"cell_type":"markdown","source":["# **Forward Propogation**"],"metadata":{"id":"lnOQqmnlw7eN"}},{"cell_type":"code","source":["def forward_prop(a, b, c) : \n","    '''\n","    Input : real numbers a, b, c.\n","\n","    Outputs : A dictionary of the values at each node with keys as the names of nodes\n","    Grads : A dictionary of the gradients at each edge with keys as a pair of nodes \n","    \n","    e.g. Grads[\"ce\"] = ...\n","    '''\n","    Outputs={}\n","    Grads={}\n","    Outputs[\"a\"]=a\n","    Outputs[\"b\"]=b\n","    Outputs[\"c\"]=c\n","\n","    A=Multiply_Scalar([\"a\",\"3\"],[a,3])\n","    d=A.compute_output()\n","    grad=A.compute_gradients()\n","    Outputs[\"d\"]=d\n","    Grads[\"ad\"]=grad[A.lst_names[0]]\n","\n","    B=Multiply([\"a\",\"b\",\"c\"],[a,b,c])\n","    e=B.compute_output()\n","    grad=B.compute_gradients()\n","    Outputs[\"e\"]=e\n","    Grads[\"ae\"]=grad[B.lst_names[0]]\n","    Grads[\"be\"]=grad[B.lst_names[1]]\n","    Grads[\"ce\"]=grad[B.lst_names[2]]\n","\n","    C=Sine(\"c\",c)\n","    f=C.compute_output()\n","    grad=C.compute_gradients()\n","    Outputs[\"f\"]=f\n","    Grads[\"cf\"]=grad[C.name]\n","\n","    D=Exponential(\"e\",e)\n","    g=D.compute_output()\n","    grad=D.compute_gradients()\n","    Outputs[\"g\"]=g\n","    Grads[\"eg\"]=grad[D.name]\n","\n","    E=Add([\"a\",\"d\", \"g\", \"f\"], [a, d, g, f])\n","    h=E.compute_output()\n","    grad=E.compute_gradients()\n","    Outputs[\"h\"]=h\n","    Grads[\"ah\"]=grad[E.lst_names[0]]\n","    Grads[\"dh\"]=grad[E.lst_names[1]]\n","    Grads[\"gh\"]=grad[E.lst_names[2]]\n","    Grads[\"fh\"]=grad[E.lst_names[3]]\n","    \n","    return (Outputs, Grads)\n","\n","print(forward_prop(1,2,3))"],"metadata":{"id":"GM2n6S4sw-r_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672512003367,"user_tz":-330,"elapsed":4,"user":{"displayName":"Sathwika Reddy somidi","userId":"01697320888258713645"}},"outputId":"cccac0f6-989f-437f-9f70-ca6c35381ebd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["({'a': 1, 'b': 2, 'c': 3, 'd': 3, 'e': 6, 'f': 0.1411200080598672, 'g': 403.4287934927351, 'h': 407.569913500795}, {'ad': 3.0, 'ae': 6.0, 'be': 3.0, 'ce': 2.0, 'cf': -0.9899924966004454, 'eg': 403.4287934927351, 'ah': 1.0, 'dh': 1.0, 'gh': 1.0, 'fh': 1.0})\n"]}]},{"cell_type":"markdown","source":["# **Backward Propogation**"],"metadata":{"id":"ApNrGkHJxjMw"}},{"cell_type":"markdown","source":["Most of the usage of computational graphs in Machine Learning centers around finding the gradients of a particular loss, wrt to all the parameters. Here, your task is to do a simpler version of that.\n","\n","Use the classes you wrote to calculate the following gradients : \n","\n","\n","*   $ \\frac{\\partial h}{\\partial a}$\n","*   $ \\frac{\\partial h}{\\partial b}$\n","*   $ \\frac{\\partial h}{\\partial c}$\n","\n","Use the technique of *backpropogation* to code out the gradients step-wise, along all possible chains of the graph starting from $h$ and ending at $a,b,c$ respectively. \n","\n","Don't try to directly get these values without backpropogation, it might be easier here, but with complicated neural networks it wouldn't be :) \n","\n"],"metadata":{"id":"450-YOv8fqmR"}},{"cell_type":"code","source":["def backward_prop(a, b, c, Outputs, Grads) : \n","    '''\n","    Input : a, b, c (input layer); Outputs (values at each node); Grads (gradients stored at each edge)\n","    Retuns : result (gradients w.r.t a, b, c)\n","    '''\n","    result=[Grads[\"ad\"]*Grads[\"dh\"], Grads[\"gh\"]*Grads[\"eg\"]*Grads[\"be\"], Grads[\"cf\"]*Grads[\"fh\"]]\n","\n","    return result"],"metadata":{"id":"YwL98tWexpGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Combining both processes**\n","\n","For the purpose of values, assume that $a = 3, b = 1, c = 2$. Here, we call both forward and backward propogation functions to demonstrate functionality of the functions above."],"metadata":{"id":"hOsnYr0lx_KT"}},{"cell_type":"code","source":["# Initialisation\n","a = 3\n","b = 1\n","c = 2\n","\n","# Forward propogation                                                                                                                                                                                  \n","(Outputs, Grads) = forward_prop(a, b, c)\n","print(f'Value obtained upon forward propogation : {Outputs[\"h\"]}')\n","\n","# Backward propogation\n","result = backward_prop(a, b, c, Outputs, Grads)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n","print(f'Values of gradients are : {result[0]}, {result[1]}, {result[2]}')"],"metadata":{"id":"cn-KFyH4yWAJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1672513033422,"user_tz":-330,"elapsed":515,"user":{"displayName":"Sathwika Reddy somidi","userId":"01697320888258713645"}},"outputId":"6a492fe4-fa6b-4362-d95c-3d0da7b8092c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Value obtained upon forward propogation : 416.3380909195608\n","Values of gradients are : 3.0, 2420.5727609564105, -0.4161468365471424\n"]}]},{"cell_type":"markdown","source":["# **Submission Instructions**\n","\n","Upload this notebook on your github classroom repository by the name Week1.ipynb"],"metadata":{"id":"iMTRGZQoy-VU"}}]}